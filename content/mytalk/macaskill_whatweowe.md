---
title: "What We Owe the Future"
summary: "Will MacAskill argues that our long term impact should be our most important moral consideration."
authors: [Will MacAskill]
tags: [Effective Altruism]
categories: []
share: false
date: 2019-10-15
math: true
---

> How can we do as much good as possible? The moral priority in the world today is to ensure a flourishing long-run future. 

1. Future people matter, morally. 

2. There are, in expectation, prodigious numbers of future people. Typical mammalian species lives for an additional 1000000 years. But why could this number not be higher? Earth will be habitable for another 100 years. Morally speaking, the numbers matter. 

3. Future people are utterly disenfranchised. They cannot influence politics today for example. Economicists have discount factors, as people prefer benefits now over benefit in the future. e.g. 1 death in 10,000 years vs 1,000,000 deaths in 11,000 years, but not if the discounting is too much. The mistake is looking at how people behave in their own lives, but this should not end up priviledging current generations. 

4. There are ways to positively impact the very long run. Do our actions fade over time? *Ozymandias problem*. But is this true for all actions? Extnction events and "locked-in values and institutions" have longer term impacts. For example, the US constution is still having a big impact. 

We are lke a swimmer trying to steer a ship.

In terms of careers, let's think about the next 50 years. People assume the future will be similar to know, but unlikely to happen? Our GDP could collapse a la Rome, or there could be a new growth mode. 

Deadliest events ever: war, pandemic & plague. Unlikely to happen due to natural pandemic, but making man-made pathogens is problematic. Contagiousness of common cold, mortality rate of Ebola and incubation period of HIV. And its becoming much cheaper to both synthesise and sequence DNA. 

Developments in Artificial Intelligence could cause a new growth mode. Supercomputer power is increasing signficantly. Aim is AGI, and the median AI researcher believes human-level AI will happen wthn the next 50 years, but wide error bars. The median research reports a 5% chance that would be as bad as human extinction. But could this create a significant concentration of power. Historically, this doesn't go well. 

How could ideology get locked in?

1. Anti-aging technology.

2. Cloning technology. We can do this already. 

3. Individuals can pass control to AI agents. 

### What can you do?
80,000 hours. Rare concentration of influential people. 