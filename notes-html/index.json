[{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m a first year PhD student at Magdalen College at the Univeristy of Oxford, in the Autonomous Intelligent Machines and Systems CDT (AIMS) programme.\nI\u0026rsquo;m broadly interested in Bayesian machine learning, believing that uncertainty quantification is essential, especially for optimal decision making. I also want to develop general techniques and algorithms which enable inference to be applied to many real-world scenarios, including including improving \u0026lsquo;robustness\u0026rsquo; (e.g. traditional adversarial robustness, as well other techniques such as differential privacy) as well as data efficiency (spanning semi-supervised learning to meta-learning).\nI also identify strongly with the effective altruism community and hope to use my career to make a positive social impact.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I\u0026rsquo;m a first year PhD student at Magdalen College at the Univeristy of Oxford, in the Autonomous Intelligent Machines and Systems CDT (AIMS) programme.\nI\u0026rsquo;m broadly interested in Bayesian machine learning, believing that uncertainty quantification is essential, especially for optimal decision making. I also want to develop general techniques and algorithms which enable inference to be applied to many real-world scenarios, including including improving \u0026lsquo;robustness\u0026rsquo; (e.g. traditional adversarial robustness, as well other techniques such as differential privacy) as well as data efficiency (spanning semi-supervised learning to meta-learning).","tags":null,"title":"Mrinank Sharma","type":"authors"},{"authors":["Mrinank Sharma"],"categories":[],"content":"Machine learning is often applied in contexts in which:\n The data is distributed across many different client devices. Think mobile phones, autonomous vehicles, Internet of Things devices \u0026hellip; The data contains to private, sensitive information about individuals, for example, healthcare data. We want to train a machine learning model to help us to make decisions i.e. we want to be Bayesian and model uncertainty.  Examples of where all three of the above hold include smartphone keyboard text prediction, and healthcare drug sensitivity analysis (i.e., which drug should be given to a particular patient).\nThe task of learning an accurate Bayesian model on distributed data is already somewhat challenging, but when individuals contribute sensitive data, there are additional concerns. Will the model predictions inadvertantly leak information about the individuals used to train the model? In fact, neural networks have been show to exhibit the property of memorising training examples 1. Given this, how can we convince new users to contribute their data? Note that we want to ensure privacy in the sense that when the full trained model is released to the public, the information about individual users is still protected.\nDifferential Privacy (DP) 2 is the current gold standard for mathematically quantifying the level of privacy offered by a randomised algorithm. Note that there is a requirement for the algorithm to be randomised, and this can be interpreted as giving each individual some plausibility deniability.\nThis project built upon Partitioned Variational Inference (PVI) 3, a recently developed framework which supports federated (distributed) variational inference, a form of approximate inference, adapting the algorithm to provide a differential privacy guarantee. An arVix paper should be appearing soon, but in the meantime, please see the Github and do not hestitate to contact me if you are interested!\n Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos, Dawn Song, Úlfar Erlingsson, and Dawn Song. The Secret Sharer: Measuring Unintended Neural Network Memorization \u0026amp; Extracting Secrets. 2018. arVix ^ Cynthia Dwork and Aaron Roth. The Algorithmic Foundations of Differential privacy. Foundations and Trends in Theoretical Computer Science, 9(3-4):211–487, 2013. ISSN 15513068. doi:10.1561 / 0400000042. ^ Thang D. Bui, Cuong V. Nguyen, Siddharth Swaroop, and Richard E. Turner. Partitioned Variational Inference: A unified framework encompassing federated and continual learning. November 2018. arXiv ^   ","date":1569283200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569283200,"objectID":"92284a80e37e97b8946af8baf871807f","permalink":"/book/privacy_project/","publishdate":"2019-09-24T00:00:00Z","relpermalink":"/book/privacy_project/","section":"book","summary":"Learning Private, Bayesian Machine Learning Models in the Federating Learning Context","tags":["Differential Privacy (DP)"],"title":"Differential Privacy and Approximate Bayesian Inference","type":"book"},{"authors":["Mrinank Sharma"],"categories":[],"content":"Machine learning is often applied in contexts in which:\n The data is distributed across many different client devices. Think mobile phones, autonomous vehicles, Internet of Things devices \u0026hellip; The data contains to private, sensitive information about individuals, for example, healthcare data. We want to train a machine learning model to help us to make decisions i.e. we want to be Bayesian and model uncertainty.  Examples of where all three of the above hold include smartphone keyboard text prediction, and healthcare drug sensitivity analysis (i.e., which drug should be given to a particular patient).\nThe task of learning an accurate Bayesian model on distributed data is already somewhat challenging, but when individuals contribute sensitive data, there are additional concerns. Will the model predictions inadvertantly leak information about the individuals used to train the model? In fact, neural networks have been show to exhibit the property of memorising training examples 1. Given this, how can we convince new users to contribute their data? Note that we want to ensure privacy in the sense that when the full trained model is released to the public, the information about individual users is still protected.\nDifferential Privacy (DP) 2 is the current gold standard for mathematically quantifying the level of privacy offered by a randomised algorithm. Note that there is a requirement for the algorithm to be randomised, and this can be interpreted as giving each individual some plausibility deniability.\nThis project built upon Partitioned Variational Inference (PVI) 3, a recently developed framework which supports federated (distributed) variational inference, a form of approximate inference, adapting the algorithm to provide a differential privacy guarantee. An arVix paper should be appearing soon, but in the meantime, please see the Github and do not hestitate to contact me if you are interested!\n Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos, Dawn Song, Úlfar Erlingsson, and Dawn Song. The Secret Sharer: Measuring Unintended Neural Network Memorization \u0026amp; Extracting Secrets. 2018. arVix ^ Cynthia Dwork and Aaron Roth. The Algorithmic Foundations of Differential privacy. Foundations and Trends in Theoretical Computer Science, 9(3-4):211–487, 2013. ISSN 15513068. doi:10.1561 / 0400000042. ^ Thang D. Bui, Cuong V. Nguyen, Siddharth Swaroop, and Richard E. Turner. Partitioned Variational Inference: A unified framework encompassing federated and continual learning. November 2018. arXiv ^   ","date":1569283200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569283200,"objectID":"f70f4d5f82cdc2030d1c5c2a877114d0","permalink":"/mytalk/privacy_project/","publishdate":"2019-09-24T00:00:00Z","relpermalink":"/mytalk/privacy_project/","section":"mytalk","summary":"Learning Private, Bayesian Machine Learning Models in the Federating Learning Context","tags":[],"title":"Differential Privacy and Approximate Bayesian Inference","type":"mytalk"},{"authors":["Mrinank Sharma"],"categories":[],"content":"Machine learning is often applied in contexts in which:\n The data is distributed across many different client devices. Think mobile phones, autonomous vehicles, Internet of Things devices \u0026hellip; The data contains to private, sensitive information about individuals, for example, healthcare data. We want to train a machine learning model to help us to make decisions i.e. we want to be Bayesian and model uncertainty.  Examples of where all three of the above hold include smartphone keyboard text prediction, and healthcare drug sensitivity analysis (i.e., which drug should be given to a particular patient).\nThe task of learning an accurate Bayesian model on distributed data is already somewhat challenging, but when individuals contribute sensitive data, there are additional concerns. Will the model predictions inadvertantly leak information about the individuals used to train the model? In fact, neural networks have been show to exhibit the property of memorising training examples 1. Given this, how can we convince new users to contribute their data? Note that we want to ensure privacy in the sense that when the full trained model is released to the public, the information about individual users is still protected.\nDifferential Privacy (DP) 2 is the current gold standard for mathematically quantifying the level of privacy offered by a randomised algorithm. Note that there is a requirement for the algorithm to be randomised, and this can be interpreted as giving each individual some plausibility deniability.\nThis project built upon Partitioned Variational Inference (PVI) 3, a recently developed framework which supports federated (distributed) variational inference, a form of approximate inference, adapting the algorithm to provide a differential privacy guarantee. An arVix paper should be appearing soon, but in the meantime, please see the Github and do not hestitate to contact me if you are interested!\n Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos, Dawn Song, Úlfar Erlingsson, and Dawn Song. The Secret Sharer: Measuring Unintended Neural Network Memorization \u0026amp; Extracting Secrets. 2018. arVix ^ Cynthia Dwork and Aaron Roth. The Algorithmic Foundations of Differential privacy. Foundations and Trends in Theoretical Computer Science, 9(3-4):211–487, 2013. ISSN 15513068. doi:10.1561 / 0400000042. ^ Thang D. Bui, Cuong V. Nguyen, Siddharth Swaroop, and Richard E. Turner. Partitioned Variational Inference: A unified framework encompassing federated and continual learning. November 2018. arXiv ^   ","date":1569283200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569283200,"objectID":"02906383c79f8357f49e2f18debed8ae","permalink":"/paper/privacy_project/","publishdate":"2019-09-24T00:00:00Z","relpermalink":"/paper/privacy_project/","section":"paper","summary":"Learning Private, Bayesian Machine Learning Models in the Federating Learning Context","tags":[],"title":"Differential Privacy and Approximate Bayesian Inference","type":"paper"}]